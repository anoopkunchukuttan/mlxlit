import numpy as np
import pandas as pd 

import matplotlib
import matplotlib.pyplot as plt

from cfilt.transliteration.analysis import align
from cfilt.transliteration import news2015_utilities as nwutil

from indicnlp.script import indic_scripts as isc 
from indicnlp.transliterate import unicode_transliterate  as indtrans

def get_column_name(x,tlang): 
    if isc.is_supported_language(tlang): 
        #return x if tlang=='hi' else indtrans.UnicodeIndicTransliterator.transliterate(x,tlang,'hi')
        if isc.in_coordinated_range(x,tlang): 
            return indtrans.ItransTransliterator.to_itrans(x,tlang) + '({:2x})'.format(isc.get_offset(x,tlang))
        else: 
            return str(hex(ord(x)))
    elif tlang=='ar': 
        pass 
        return x
    else: 
        return x

def plot_confusion_matrix(confusion_mat_fname,tlang,image_fname):
    """
    Plots a heat map of the confusion matrix of character alignments. 
    
    confusion_mat_fname: The input is a confusion matrix generated by the align script (csv format)
    tgt: target language of the transliteration
    
    For Indic scripts, the heat map shows characters in Devanagiri irrespective of target language for readability
        - Needs 'Lohit Devanagari' font to be installed 
    """
    
    #matplotlib.rc('font', family='Lohit Kannada') 
    matplotlib.rcParams.update({'font.size': 8})

    confusion_df=pd.read_pickle(confusion_mat_fname)

    schar=list(confusion_df.index)
    tchar=list(confusion_df.columns)
    i=0
    for c in schar: 
        if c in tchar: 
            confusion_df.ix[c,c]=0.0
    
    data=confusion_df.as_matrix()
    
    # # normalize along row
    # sums=np.sum(data,axis=1)
    # data=data.T/sums
    # data=data.T
    
    # # normalize along column
    # sums=np.sum(data,axis=0)
    # data=data/sums
    
    #s=np.sum(data)
    #data=data/s
    
    columns=list(confusion_df.columns)
    col_names=[ get_column_name(x,tlang) for x in columns]

    rows=list(confusion_df.index)
    row_names=[ get_column_name(x,tlang) for x in rows]
    
    plt.figure(figsize=(20,10))

    #plt.pcolor(data,cmap=plt.cm.gray_r,edgecolors='k')
    plt.pcolor(data,cmap=plt.cm.hot_r,edgecolors='k')
    
    #plt.pcolor(data,edgecolors='k')
    plt.colorbar()
    plt.xticks(np.arange(0,len(col_names))+0.5,col_names,rotation='vertical')
    plt.yticks(np.arange(0,len(row_names))+0.5,row_names)
    plt.xlabel('system')
    plt.ylabel('reference')
    
    plt.savefig(image_fname)
    plt.close()

def transliteration_analysis(exp_dirname,epoch,ref_fname,slang,tlang):
    """
    exp_dirname: base directory of the experiment 
    epoch: model to use - indicated by the epoch number 
    ref_fname: reference file 
    slang: source langauge 
    tlang: target language 
    """
   
    ## generate file names 
    out_fname='{exp_dirname}/outputs/{epoch:03d}test.nbest.{slang}-{tlang}.{tlang}'.format(
        exp_dirname=exp_dirname,epoch=epoch,slang=slang,tlang=tlang)
    out1b_fname='{exp_dirname}/outputs/{epoch:03d}test.1best.{slang}-{tlang}.{tlang}'.format(
        exp_dirname=exp_dirname,epoch=epoch,slang=slang,tlang=tlang)

    ## save the output 
    nwutil.convert_to_1best_format(out_fname,out1b_fname)
    out_dirname='{exp_dirname}/outputs/{epoch:03d}_analysis_{slang}-{tlang}'.format(
        exp_dirname=exp_dirname,epoch=epoch,slang=slang,tlang=tlang)
    align.save_analysis_artifacts(ref_fname, out1b_fname, tlang, out_dirname)

    ## plot the confusion matrix  
    confmat_fname='{exp_dirname}/outputs/{epoch:03d}_analysis_{slang}-{tlang}/confusion_mat.pickle'.format( 
        exp_dirname=exp_dirname,epoch=epoch,slang=slang,tlang=tlang) 
    confmat_img_fname='{exp_dirname}/outputs/{epoch:03d}_analysis_{slang}-{tlang}/confusion_mat.png'.format( 
        exp_dirname=exp_dirname,epoch=epoch,slang=slang,tlang=tlang) 
    plot_confusion_matrix(confmat_fname,tlang,confmat_img_fname)


def run_generate_analysis(basedir,exp_conf_fname): 
    """
     Run experiments to generate analysis files 
    """

    #def should_do_exp(rec): 
    #    """
    #    Filtering the list of experiments: this needs to be configured while running experiments 
    #    """
    #
    #    exp_check = rec['exp'] in ['2_multilingual','2_bilingual'] 
    #
    #    ## krishna
    #    #dataset_check = rec['dataset'] in ['ar-slavic_latin', 'news_2015_reversed'] 
    #
    #    ### balaram 
    #    #dataset_check = rec['dataset'] in ['slavic_latin-ar', 'news_2015_indic', 'news_2015_official' ] 
    #    dataset_check = rec['dataset'] in ['news_2015_indic', 'news_2015_official' ] 
    #
    #    return exp_check and dataset_check 

    #def should_do_exp(rec): 
    #    """
    #     Always do experiments 
    #    """
    #    return True 

    def should_do_exp(rec): 
        """
         Never do experiments 
        """
        return False 
    

    def get_edir(rec): 
        """
         get the name of the basedir for the experiment 
        """
    
        if (rec['exp'].find('bilingual')>=0) or (rec['exp'].find('moses')>=0) : 
            return '{}-{}'.format(rec['src'],rec['tgt'])

        elif rec['exp'].find('multilingual')>=0: 
            if rec['dataset'] in ['ar-slavic_latin','slavic_latin-ar']: 
                return 'multi-conf'
            elif rec['dataset'] == 'news_2015_official' : 
                return 'en-indic'
            elif rec['dataset'] == 'news_2015_reversed' : 
                return 'indic-en'
            elif rec['dataset'] == 'news_2015_indic' : 
                return 'indic-indic'
            else: 
                print 'Unknown experiment' 

        else: 
            print 'Invalid configuration'

    ## read the list of experiments to be analyzed 
    print 'Read list of experiments' 
    conf_df=pd.read_csv(exp_conf_fname,header=0,sep=',')

    
    for rec in filter( should_do_exp, [x[1] for x in conf_df.iterrows()]): 

        edir=get_edir(rec)

        exp_dirname = '{basedir}/results/sup/{dataset}/{exp}/{rep}/{edir}'.format(
                basedir=basedir,dataset=rec['dataset'],rep=rec['representation'],exp=rec['exp'],edir=edir)

        ref_fname = '{basedir}/data/sup/mosesformat/{dataset}/{slang}-{tlang}/test.{tlang}'.format(
                basedir=basedir,dataset=rec['dataset'],slang=rec['src'],tlang=rec['tgt'])

        print 'Starting Experiment: ' + exp_dirname
        transliteration_analysis(exp_dirname,rec['epoch'],ref_fname,rec['src'],rec['tgt'])
        print 'Finished Experiment: ' + exp_dirname
        sys.stdout.flush()

if __name__ == '__main__': 
   
    run_generate_analysis('/home/development/anoop/experiments/multilingual_unsup_xlit','results_with_accuracy.csv') 

    #datasets=[
    #                 'ar-slavic_latin',
    #                 'slavic_latin-ar',
    #                 'news_2015_official',
    #                 'news_2015_indic',
    #                 'news_2015_reversed',
    #             ]

    #transliteration_analysis(
    #        '/home/development/anoop/experiments/multilingual_unsup_xlit/results/sup/news_2015_indic/2_multilingual/onehot_shared/indic-indic',
    #        11,
    #        '/home/development/anoop/experiments/multilingual_unsup_xlit/data/sup/mosesformat/news_2015_indic/hi-bn/test.bn',
    #        'hi',
    #        'bn',
    #        )
        
